%
% File nodalida2017.tex
%
% Contact beata.megyesi@lingfil.uu.se
%
% Based on the instruction file for Nodalida 2015 and EACL 2014
% which in turn was based on the instruction files for previous 
% ACL and EACL conferences.

\documentclass[11pt]{article}
\usepackage{nodalida2017}
%\usepackage{times}
\usepackage{mathptmx}
%\usepackage{txfonts}
\usepackage{url}
\usepackage{latexsym}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to

\title{Cleaning up the Basque grammar: a work in progress}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\}

\def\t#1{\texttt{#1}}

\date{}

\begin{document}
\maketitle

% \begin{abstract}
%   This document contains the instructions for preparing a camera-ready
%   manuscript for the proceedings of Nodalida-2017. The document itself
%   conforms to its own specifications, and is therefore an example of
%   what your manuscript should look like. These instructions should be
%   used for both papers submitted for review and for final versions of
%   accepted papers.  Authors are asked to conform to all the directions
%   reported in this document.
% \end{abstract}


\section{Introduction}

The first version of the Basque Constraint Grammar (BCG) was developed in 1995--1997 by two linguists \cite{aduriz1997euscg} based on the Constraint Grammar theory of \newcite{karlsson1990cgp,karlsson1995constraint}.
Since then, it has undergone many changes, by many grammarians.
% The file consists of 8600 lines, out of which 2600 are rules or tagsets--rest are comments and commented out rules or sets. 
During the two decades of development, the Basque morphological analyser has also been updated several times, and not always synchronised with the CG. As a result, the Basque grammar needs serious attention.

In the present paper, we describe the ongoing process of cleaning up the Basque grammar. We use a variety of tools and methods, ranging from simple string replacements to SAT-based symbolic evaluation, introduced in \newcite{listenmaa_claessen2016}, and grammar tuning by \newcite{bick2013tuning}. We present our experiences in combining all these tools, along with a few modest additions to the simpler end of the scale.


\section{Previous work}

\newcite{bick2013tuning} presents a method for automatically tuning a grammar, and reports an error reduction between 7--15 \% when tested on the Danish tagging grammar.
\newcite{listenmaa_claessen2016} present a method for detecting contradictions in a grammar, using SAT-based symbolic evaluation.  They report detecting rule conflicts in a few small grammars, but provide no further evaluation on the grammars after fixing the rule conflicts. 
In our experiments, we use both of these tools for different purposes, complementing each other.

\section{Pipeline}

As a first step, we run a series of simple, mostly off-the-shelf tools. 
The next step is to group the rules and order them by their contextual tests.
These sets are checked both by the SAT-based tool, and grammarians. After these
steps, we give the grammar as an input for ML-tuning.


\begin{figure*}[t]
\centering
\label{fig:sorted}
\begin{tabular}{lr}
\t{SELECT ADOIN IF (1 ARAZI) ;}                         & \t{\# line 7412} \\
\t{REMOVE ADOIN IF (0 IZE) (1C ADJ) ;}                  & \t{\# line 6423} \\
\t{REMOVE ADOIN IF (0 IZE) (1 DET | ADJ | IZE) ;}       & \t{\# line 6433} \\
\t{REMOVE ADOIN IF (0 EZEZAG + ADJ + GEN) (-2C IZE) ;}  & \t{\# line 6319} \\
\t{REMOVE ADOIN IF (0 IZE) (-1C IZE) (1C ADJ) ;}        & \t{\# line 6422} \\
\end{tabular}
\caption{Rules grouped by target, and ordered by their contextual tests.}

\end{figure*}



\subsection{Simple tools}

\paragraph{String operations}
Fix typos: O for 0, and various mismatched \texttt{"<>"} in word forms: e.g. \texttt{"<zuen">}, \texttt{<argi>"}.
Transform word forms into case-insensitive, remove duplicates. 
There were many occurrences of identical rules, of the form \texttt{REMOVE ("<x>")} and \texttt{REMOVE ("<X>")}. We changed those rules into the form \texttt{REMOVE ("<x>"i)}, and removed duplicate rules after that.

\paragraph{Tagset operations}
The VISL CG-3 compiler offers useful features, such as \texttt{--show-unused-sets} and \texttt{--show-tags}. With the former, we could eliminate 255 unused tagsets, and with the latter, we detected 15 obsolete or misspelled tags in the remaining used tagsets, by comparing against an up-to-date morphological lexicon.

% A special case, which occurred 4 of the 15 times, was to use a set in place of a tag name, such as in Figure~\ref{fig:tagnames}.

% \begin{figure}
% \caption{Tagsets used as tag names}
% \label{fig:tagnames}
% \begin{verbatim}
% LIST ADI/ADT = ADI ADT ;
% LIST ADIZEHAR = ("jakin" ADI/ADT) ;

% LIST MOD/DENB = MOD DENB ;
% REMOVE (BALD ERL MOD/DENB) ;
% \end{verbatim}
% \end{figure}

% Both of the lists, ADIZEHAR and the inline target in the remove rule, are using a previously defined list in a position where there should be a tag name instead. As a result, ADI/ADT and MOD/DENB are interpreted as tag names---such tag names don't exist, and hence these rules can never apply.

\paragraph{Human readability}
For improving the readability of the grammar, we wrote a tool that finds repetitive set definitions, and suggests ways to compact them. An example is shown below:

\begin{table}[h]
\begin{tabular}{l}
Original  \\
 ~~~~~~~~\texttt{("ageri" ADJ ABS MG)} \\
 ~~~~~~~~\texttt{("bizi" ADJ ABS MG) ...} \\
 ~~~~~~~~\texttt{("haizu" ADJ ABS MG) ;} \\
Compact \\
 ~~~~~~~~\texttt{("ageri"|"bizi"|"haizu") +} \\
 ~~~~~~~~\texttt{(ADJ ABS MG) ;}

\end{tabular}
\end{table}


In addition, the grammar contains many rules that specify an inline set, when there is already the same or a very similar set definition. For instance, the rule \texttt{REMOVE (ADL) IF (0 ADT) (1 ("<.>") OR ("<;>") OR ("<,>") OR ("<:>") OR ("<?>") OR ("<!>"))} lists different punctuation marks as word forms, instead of using the list PUNTUAZIOA, which contains all these tokens.

The standard tools did not provide this type of suggestions, so we wrote these tools ourselves. Neither of these transformations is applied automatically, they are just suggestions for the grammar writers.



\subsection{Group by target, sort by conditions}
After the simple checks and transformations, we group the rules by their targets, and sort them by the complexity of their contextual texts. For instance, the 5 rules that target ADOIN will be in the order shown in Figure~\ref{fig:sorted}: from fewest to most contextual tests, and in the case of same number of tests, preferring those with fewer tagsets.


\subsection{Check for conflicts and redundancies}

When the rules are grouped and sorted as described, we run SAT-based 
symbolic evaluation \cite{listenmaa_claessen2016} on each group. 
If it says that some rule with a more complex condition is superfluous 
because of another rule earlier in the list\footnote{For example, the latter rule of the following is superfluous: \texttt{REMOVE Verb IF (-1 Det)} and \texttt{REMOVE Verb IF (-1 Det) (1 Verb)}}, 
then that is a hint for the grammar writer: why are there two similar 
rules in the grammar, if the simpler one would do?
At the moment of writing, we are still working on adding
the SAT-method to the pipeline, and have only preliminary results to report.

Even if the SAT-method wouldn't find a conflict, we give the rules to 
a grammarian in any case. The grammarian works with this list, having 
the original grammar on the side to see the comments, or other original 
context of any given rule. Seeing all the rules grouped helps with the 
situation where different grammarians have written rules independent of 
each other. The grammarian can do their manual modifications either on 
the original grammar, or to this grouped version.

This sorting and grouping is not meant to be the final order, it is only 
to help a human grammarian to make decisions regarding all the rules that 
target the same tagsets.

\subsection{Ordering: ML-tuning}
Once the grammar has gone through the previous steps, we give it to the 
ML-tuning tool \cite{bick2013tuning}, with the purpose of finding an 
optimal order. Then we can run the newly ordered grammar through SAT-evaluation,
to detect if the ML-tuning has introduced new conflicts or superfluous rules.

Our initial hypothesis is that the human-cleaned version will benefit more 
from tuning than the original grammar. Some bad rules may have only a minor 
problem, such as a single tag name having changed meaning, and they would be 
better fixed by updating the obsolete tag, instead of the whole rule being 
demoted or killed. 
To test our assumptions, we run ML-tuning for both the original grammar and 
the human-cleaned version: it is also possible that this error type is not very 
common, and the majority of the bad rules were bad already when they were born.

% We are also interested to find out whether there are some rules or rule types 
% that are consistently missed by one of the tuning methods. 

%We compare the performance of the grammar in its original state, and the cleaned up version. 

\section{Evaluation}

We evaluate the grammars with a manually disambiguated corpus of 65,153 tokens/53,429 words,
compiled from different sources \cite{aduriz2006epec}.
We report the original score, and the result from ML-tuning the original grammar, 
as well as the result of preliminary cleanup. 
%Since our work is very much in progress, the latter grammar has lower F-score than the original.
The scores are given using two metrics, differing on the granularity of the tagset.

\subsection{Evaluation criteria}

The Basque tag set is structured in four levels of granularity. 
As explained in \newcite{ezeiza1998basque}, the first level contains only the main POS, 20 distinct tags,
and the fourth level contains several hundreds of tags with fine-grained distinctions, including semantic tags such as animacy.
Table~\ref{table:levels} shows a simplified example of the levels for noun.
On the 4th level, the initial ambiguity is very high: the test corpus has, on the average,
\textbf{XXX} readings per cohort. On the 2nd level, when readings that differ only in 
higher-level tags are collapsed into one, the initial ambiguity is \textbf{YYY} readings per cohort.
We follow the scheme for evaluation: assume that we are left with two readings, ``Common noun, singular''
and ``Common noun, plural'', and one of them is correct. Evaluation on levels 3 and 4 reports 100 \% recall and 50 \% precision.
Evaluation on levels 1 and 2 ignores the tags from the higher levels, and regards any common noun or noun as correct, hence 100 \% for both measures. 

%the 8 noun readings on level 4 would be collapsed to four readings on level 3, two on level 2 and one on level 1.
%As for evaluation, any correct noun reading would give 100%  precision and recall on level 1, and 50% on level 2. 

\begin{table*}[t]
\centering
\begin{tabular}{llll}

\hline
\multicolumn{1}{|l|}{\textbf{Level 1}} & \multicolumn{1}{l|}{\textbf{Level 2}}                                                  & \multicolumn{1}{l|}{\textbf{Level 3}}                                                                                                                                                          & \multicolumn{1}{l|}{\textbf{Level 4}}                                                                                                                                                                                                        \\ \hline
\multicolumn{1}{|l|}{Noun}             & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Common noun\\ Proper noun\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Common noun, plural absolutive\\ Common noun, singular ergative\\ Proper noun, plural absolutive\\ Proper noun, singular ergative\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Common noun, plural absolutive, animate\\ Common noun, plural absolutive, inanimate\\ ...\\ Proper noun, singular ergative, animate\\ Proper noun, singular ergative, inanimate\end{tabular}} \\ \hline

\end{tabular}
\caption{Levels of granularity}
\label{table:levels}
\end{table*}



\subsection{Analysis of the results}

The results of the preliminary evaluation are in Table~\ref{table:res}. 
The grammar shows worse performance after a preliminary cleanup---most likely the result is due to ordering. 
We found it easier to work on the grammar after grouping and sorting the rules, and while
the cleanup is still in progress, we did not settle to a final order yet. The fourth row of Table~\ref{table:res} shows the 
ML-tuning applied to the pre-cleaned, initially sorted grammar. We could've tried to mimic the original order
after the initial steps, but we chose not to spend such an effort at the current stage of the work. 
Instead, we keep working on the task on two fronts: the human labour driven by linguistic insight, 
and adapting the computational techniques to reveal more details.




%Given that recall only improves 2 percentage points, but precision more than 20, the main problem
%seems to be not disambiguating enough, and to only smaller extent removing the right reading.




\begin{table*}[t]
\centering
\label{my-label}
\begin{tabular}{l|l|l|l|l|l|l|}
\cline{2-7}
                                                 & \multicolumn{3}{l|}{\textbf{All tags} (Level 4)}            & \multicolumn{3}{l|}{\textbf{50 main categories} (Level 2)}  \\ \cline{2-7} 
                                                 & \textit{Rec.} & \textit{Prec.} & \textit{F-score} & \textit{Rec.} & \textit{Prec.} & \textit{F-score} \\ \hline
\multicolumn{1}{|l|}{\textbf{Original grammar--Numbers before we did anything}}     & ??       & ??        & ??    &  ??        &  ??        &  ??           \\ \hline
\multicolumn{1}{|l|}{\textbf{ML-tuned original}}     & ??       & ??        & ??    &  ??        &  ??         &  ??           \\ \hline
\multicolumn{1}{|l|}{\textbf{Preliminary cleanup---TODO: latest numbers}}  & 95.30       & 63.35        & 76.11    &  97.30        &  84.69         &  90.56           \\ \hline
\multicolumn{1}{|l|}{\textbf{ML-tuned prel.cl.}}       & 93.41       & 68.61        & 79.11    &  96.41        &  87.19         &  91.57           \\ \hline
\end{tabular}
\caption{Baseline evaluation with the two levels.}
\label{table:res}
\end{table*}



\paragraph{Conflict check}

%We have worked on a reduced set of 21,000 readings, tailored to address this particular grammar. 
%Unfortunately, 21,000 readings is at the limit of practical: the program can check up to 200 rules at a time, before it uses too much memory on a 4-year-old Macbook Air (1,7 GHz Intel Core i7, 8 GB memory). 

Our main problem is the size of the tag set: all possible combinations of tags on level 4 amount to millions of readings,
and that would make the SAT-problems too big.
We cannot just ignore all tags beyond level 2 or 3, because many of the rules rely on them as contexts.

As a first approximation, we have created a reduced set of 21000 readings, which allows the program to check up to 200 
rules at a time before running out of memory. We are still developing better solutions, and have not run the whole grammar with this setup.
Among the first rules we tested, it has found a few redundancies, such as the following:

\begin{verbatim}
  SELECT ADB IF
    (0 POSTPOSIZIOAK-9) 
    (-1 IZE-DET-IOR-ADJ-ELI-SIG + INE) ;
  SELECT ADB IF  
    (0 ("<barrena>")) (-1 INE) ;
\end{verbatim}

\noindent The problem is that the set \texttt{POSTPOSIZIOAK-9} contains the word form ``barrena'', and the other set contains the tag \texttt{INE}; in other words, the latter rule is fully contained in the first rule and hence redundant.


Our second strategy is to reduce the rules themselves: from a rule such as \texttt{SELECT Verb + Sg IF (1 Noun + Sg)},
we just remove all tags higher than level 2, resulting in \texttt{SELECT Verb IF (1 Noun)}. We also keep all lexical tags intact: 
we modified the scheme from the previous experiments \cite{listenmaa_claessen2016} and allow lexical forms to attach freely 
to any morphological tags, so allowing some hundreds or thousands of lexical forms does not create an explosion of SAT-variables.
This setup analyses the whole grammar, in the given order, in approximately 1 hour. 
With the reduced rules, the program would not find the redundancy described earlier, because
the problem lies in the 3rd-level tag \texttt{INE}. But this approximation found successfully 11 duplicates or near-duplicates in the whole grammar, such as the following:

\begin{verbatim}
SELECT IZE IF 	 # line 817
  (0 POSTPOSIZIOAK-10IZE LINK 0 IZE_ABS_MG) 
  (-1 IZE-DET-IOR-ADJ-ELI-SIG + GEN) ; 
SELECT IZE IF 	 # line 829
  (0 POSTPOSIZIOAK-10IZE + IZE_ABS_MG) 
  (-1 IZE-DET-IOR-ADJ-ELI-SIG + GEN) ;
\end{verbatim}

\noindent Both of the contextual tests contain 3rd-level tags (\texttt{ABS, MG, GEN}), but removing them keeps the sets identical, hence it is not a problem for the conflict check.



Finally, all setups have found some internal conflicts. In order to get a more reliable account, we would need 
more accurate tagset, beyond the 21000. But on the other hand, many internal conflicts can be detected by simpler means:
using STRICT-TAGS would reveal illegal tags, which are the reason for a large number of internal conflicts. 
But some cases are due to a mistake in logic, rather than a typo; examples such as the following are easily found by the tool. %elow are some examples of the internal conflicts found by the tool:

\begin{verbatim}
  REMOVE ADI IF (NOT 0 ADI) (1 BAT) ;
  SELECT ADI IF (0C ADI LINK 0 IZE) ;
\end{verbatim}

\noindent The first rule is clearly an error; it is impossible to remove an ADI from a reading that does not have one.
The conflict likely stems from a confusion between \texttt{NOT X} and \texttt{(*) - X};
The second rule is not obvious to the eye; the interplay of \texttt{0C} and \texttt{LINK 0} requires 
\texttt{ADI} and \texttt{IZE} in the same reading, which is not possible\footnote{ADI is a verb, IZE is a noun}.

%We have found the conflict check helpful for the grammarians: it is useful information to know about redundancies. But so far

\paragraph{ML-tuning}

Undoubtedly, ML-tuning is the method with the best improvement in the F-score. 
Previous experiments on ML-tuning \cite{bick2013tuning,bick_hagen2015obt} report something TODO; we agree or disagree with them.
While ML-tuning improves the grammar's performance, it makes it less readable for human eyes.
It is hard to continue development after submitting the grammar to a black box and getting the output.


\section{Wishlist / Future work}
 \textbf{The following section is just a draft, I will work on it still! If you have comments regarding the content, or suggestions for additions, or your own wishlist, you can comment on that too. But don't worry too much about the quality of writing or presentation, it's just random thoughts scattered around!}

%The following techniques are for now just speculation.

%They can work either with a corpus, or a good description of possible ambiguity classes in the language.

\subsection{Minimisation of tags}


Take a test corpus/ambiguity classes, look at the readings, 
group tags by those that discriminate between correct and incorrect readings.
Example:

\begin{verbatim}
"<lurtarraren>"
  "lurtar" ADJ ARR IZAUR+ GEN 
           NUMS MUGM ZERO <Correct!>
  "lurtar" IZE ARR GEN NUMS MUGM ZERO 
  "lurtar" ADJ ARR IZAUR+ ABS MG
\end{verbatim}

For the given cohort, tags that are only in the correct are \t{GEN} and only in incorrect are \t{IZE, ABS, MG}.
In other words, we learn that a rule that would target e.g. \t{ZERO} or \t{IZAUR+} would not remove all ambiguity.
We can compute these tags for all cohorts/ambiguity classes, and see if some rules in the grammar could be simplified.


\subsection{Views on grammar}

There are many ways to order the rules in a grammar. For better maintainability, 
it makes sense to place all rules that target certain phenomena together.
On the other hand, some rules may be very heuristic, and don't make sense to be in the same section.

We propose a solution of different \emph{views} on the grammar.
\begin{itemize}
\item Running order: can be optimised with ML-tuning, no need to be human-readable
\item Sorted by feature: for the ease of the grammarian to find all rules that target the same phenomena.
For example, ``all rules that target nouns'', ``all rules that mention postpositions in their contextual tests''.
\item Tree view of rule dependencies. See next section about that.
\end{itemize}



\subsubsection{Tree view of rule dependencies}

Two rules may be connected in several ways:

\begin{itemize}
\item Contradiction: Rules X and Y, in any order, give explicitly different instructions. Example: X = \t{REMOVE Noun} and Y = \t{SELECT Noun}.
\item ``Feeding order'': Rule X earlier in the sequence enables a later rule Y. Example: X = \t{REMOVE Noun} and Y = \t{SELECT Adj IF (-1 NOT Noun)}
\item ``Bleeding order'': Rule X earlier in the sequence blocks a later rule Y. Example:  X = \t{REMOVE Noun} and Y = \t{SELECT Adj IF (-1 Noun)}.
\item Counter(f/bl)eeding order: rules X and Y are reversed. Counterbleeding order is optimal, given that both rules are good: only in this order, both Y and X have a chance to apply. Counterfeeding is suboptimal but tolerable: X will act on the first iteration, and Y only on the second iteration, unless there is another rule that would block Y before.
\item Rule X removes both correct and incorrect readings. Rule Y, if placed before X, renders X unable to act in some cases. Ideally, in the incorrect cases!
\end{itemize}

The rules could be arranged in the form of a tree, based on how they affect and are affected by other rules. Combined with the feature view, a grammarian could have a look at all rules that target nouns and their dependencies.








% \section*{Acknowledgments}

% Do not number the acknowledgment section. Do not include this section
% when submitting your paper for review.


\bibliographystyle{acl}
\bibliography{cg}


\end{document}
