%
% File nodalida2017.tex
%
% Contact beata.megyesi@lingfil.uu.se
%
% Based on the instruction file for Nodalida 2015 and EACL 2014
% which in turn was based on the instruction files for previous 
% ACL and EACL conferences.

\documentclass[11pt]{article}
\usepackage{nodalida2017}
%\usepackage{times}
\usepackage{mathptmx}
%\usepackage{txfonts}
\usepackage{url}
\usepackage{latexsym}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to

\title{Cleaning up the Basque grammar: a work in progress}

 \author{Inari Listenmaa \\
   {\small University of Gothenburg} \\
   {\tt \small inari@chalmers.se} \\\And
   Jose Maria Arriola \\
   {\small University of the Basque Country} \\
   {\tt \small josemaria.arriola@ehu.eus} \\ \And
   Itziar Aduriz \\
   {\small University of Barcelona} \\
   {\tt \small itziar.aduriz@ub.edu} \\ \And
   Eckhard Bick \\
   {\small University of Southern Denmark} \\
   {\tt \small eckhard.bick@mail.dk} \\   
 }

\def\t#1{\texttt{#1}}

\date{}

\begin{document}
\maketitle

% \begin{abstract}
%   This document contains the instructions for preparing a camera-ready
%   manuscript for the proceedings of Nodalida-2017. The document itself
%   conforms to its own specifications, and is therefore an example of
%   what your manuscript should look like. These instructions should be
%   used for both papers submitted for review and for final versions of
%   accepted papers.  Authors are asked to conform to all the directions
%   reported in this document.
% \end{abstract}


\section{Introduction}

The first version of the Basque Constraint Grammar (BCG) was developed in 1995--1997 by two linguists \cite{aduriz1997euscg} based on the Constraint Grammar theory of \newcite{karlsson1990cgp,karlsson1995constraint}.
Since then, it has undergone many changes, by many grammarians.
% The file consists of 8600 lines, out of which 2600 are rules or tagsets--rest are comments and commented out rules or sets. 
During the two decades of development, the Basque morphological analyser has also been updated several times, and not always synchronised with the CG. As a result, the Basque grammar needs serious attention.

In the present paper, we describe the ongoing process of cleaning up the Basque grammar. We use a variety of tools and methods, ranging from simple string replacements to SAT-based symbolic evaluation, introduced in \newcite{listenmaa_claessen2016}, and grammar tuning by \newcite{bick2013tuning}. We present our experiences in combining all these tools, along with a few modest additions to the simpler end of the scale.


\section{Previous work}

\newcite{bick2013tuning} presents a method for automatically tuning a grammar, and reports an error reduction between 7--15 \% when tested on the Danish tagging grammar.
\newcite{listenmaa_claessen2016} present a method for detecting contradictions in a grammar, using SAT-based symbolic evaluation.  They report detecting rule conflicts in a few small grammars, but provide no further evaluation on the grammars after fixing the rule conflicts. 
In our experiments, we use both of these tools for different purposes, complementing each other.

\section{Pipeline}

As a first step, we run a series of simple, mostly off-the-shelf tools. 
The next step is to group the rules and order them by their contextual tests.
These sets are checked both by the SAT-based tool, and grammarians. After these
steps, we give the grammar as an input for ML-tuning.


\begin{figure*}[t]
\centering
\begin{tabular}{lr}
\t{SELECT ADOIN IF (1 ARAZI) ;}                         & \t{\# line 7412} \\
\t{REMOVE ADOIN IF (0 IZE) (1C ADJ) ;}                  & \t{\# line 6423} \\
\t{REMOVE ADOIN IF (0 IZE) (1 DET | ADJ | IZE) ;}       & \t{\# line 6433} \\
\t{REMOVE ADOIN IF (0 EZEZAG + ADJ + GEN) (-2C IZE) ;}  & \t{\# line 6319} \\
\t{REMOVE ADOIN IF (0 IZE) (-1C IZE) (1C ADJ) ;}        & \t{\# line 6422} \\
\end{tabular}
\caption{Rules grouped by target, and ordered by their contextual tests.}
\label{fig:sorted}
\end{figure*}



\subsection{Simple tools}

\paragraph{String operations}
Fix typos: O for 0, and various mismatched \texttt{"<>"} in word forms: e.g. \texttt{"<zuen">}, \texttt{<argi>"}.
Transform word forms into case-insensitive, remove duplicates. 
There were many occurrences of identical rules, of the form \texttt{REMOVE ("<x>")} and \texttt{REMOVE ("<X>")}. We changed those rules into the form \texttt{REMOVE ("<x>"i)}, and removed duplicate rules after that.

\paragraph{Tagset operations}
The VISL CG-3 compiler offers useful features, such as \texttt{--show-unused-sets} and \texttt{--show-tags}. With the former, we could eliminate 255 unused tagsets, and with the latter, we detected 15 obsolete or misspelled tags in the remaining used tagsets, by comparing against an up-to-date lexical database \cite{aldezabal2001edbl}.

% A special case, which occurred 4 of the 15 times, was to use a set in place of a tag name, such as in Figure~\ref{fig:tagnames}.

% \begin{figure}
% \caption{Tagsets used as tag names}
% \label{fig:tagnames}
% \begin{verbatim}
% LIST ADI/ADT = ADI ADT ;
% LIST ADIZEHAR = ("jakin" ADI/ADT) ;

% LIST MOD/DENB = MOD DENB ;
% REMOVE (BALD ERL MOD/DENB) ;
% \end{verbatim}
% \end{figure}

% Both of the lists, ADIZEHAR and the inline target in the remove rule, are using a previously defined list in a position where there should be a tag name instead. As a result, ADI/ADT and MOD/DENB are interpreted as tag names---such tag names don't exist, and hence these rules can never apply.

\paragraph{Human readability}
For improving the readability of the grammar, we wrote a tool that finds repetitive set definitions, and suggests ways to compact them. An example is shown below:

\begin{table}[h]
\begin{tabular}{l}
Original  \\
 ~~~~~~~~\texttt{("ageri" ADJ ABS MG)} \\
 ~~~~~~~~\texttt{("bizi" ADJ ABS MG) ...} \\
 ~~~~~~~~\texttt{("haizu" ADJ ABS MG) ;} \\
Compact \\
 ~~~~~~~~\texttt{("ageri"|"bizi"|"haizu") +} \\
 ~~~~~~~~\texttt{(ADJ ABS MG) ;}

\end{tabular}
\end{table}


In addition, the grammar contains many rules that specify an inline set, when there is already the same or a very similar set definition. For instance, the rule \texttt{REMOVE (ADL) IF (0 ADT) (1 ("<.>") OR ("<;>") OR ("<,>") OR ("<:>") OR ("<?>") OR ("<!>"))} lists different punctuation marks as word forms, instead of using the list PUNTUAZIOA, which contains all these tokens.

The standard tools did not provide this type of suggestions, so we wrote these tools ourselves. Neither of these transformations is applied automatically, they are just suggestions for the grammar writers.



\subsection{Group by target, sort by conditions}
After the simple checks and transformations, we group the rules by their targets, and sort them by the complexity of their contextual texts. For instance, the 5 rules that target ADOIN will be in the order shown in Figure~\ref{fig:sorted}: from fewest to most contextual tests, and in the case of same number of tests, preferring those with fewer tagsets.


\subsection{Check for conflicts and redundancies}

When the rules are grouped and sorted as described, we run SAT-based 
symbolic evaluation \cite{listenmaa_claessen2016} on each group. 
If it says that some rule with a more complex condition is superfluous 
because of another rule earlier in the list\footnote{For example, the latter rule of the following is superfluous: \texttt{REMOVE Verb IF (-1 Det)} and \texttt{REMOVE Verb IF (-1 Det) (1 Verb)}}, 
then that is a hint for the grammar writer: why are there two similar 
rules in the grammar, if the simpler one would do?
At the moment of writing, we are still looking for better ways to adapt the conflict check
to the Basque grammar; due to the large number of tags, we cannot use the system out of the box.
We describe the adaptations we have done so far in Section~\ref{analysis},
as well as some preliminary results.

\subsection{Manual cleanup}
\label{manual} 

Even if the program wouldn't find any conflicts, we give the rules to 
a grammarian in any case. The grammarian works with this list, having 
the original grammar on the side to see the comments, or other original 
context of any given rule. 
On the one hand, seeing all the rules grouped helps with the 
situation where different grammarians have written rules independent of 
each other. On the other hand, working with the ordered grammar makes
it difficult to compare the precision, recall and F-score to the
original grammar in the intermediate stages of the cleanup. 

In any case, the sorting and grouping is not meant to be the final order, it is only 
to help a human grammarian to make decisions regarding all the rules that 
target the same tagsets.
An easy alternative would be to work on the original grammar instead, only
keeping the sorted list as a help and generating new ones as the
cleanup proceeds. However, we found it easier to work directly on the
sorted list. To solve the problem of intermediate evaluation, we
decided to compare the results by ML-tuning both the original and the
ordered grammar.

\subsection{ML-tuning}
Once the grammar has gone through the previous steps, we give it to the 
ML-tuning tool \cite{bick2013tuning}, with the purpose of finding an 
optimal order. Then we can run the newly ordered grammar through the
conflict check, to detect if the ML-tuning has introduced new
conflicts or superfluous rules.

Our initial hypothesis is that the human-cleaned version will benefit more 
from tuning than the original grammar. Some bad rules may have only a minor 
problem, such as a single tag name having changed meaning, and they would be 
better fixed by updating the obsolete tag, instead of the whole rule being 
demoted or killed. To test our assumptions, we tune both the original
grammar and the human-cleaned versions, continuously comparing the new
versions to the original.
% it is also possible that this error type is not very 
%common, and the majority of the bad rules were bad already when they were born.

% We are also interested to find out whether there are some rules or rule types 
% that are consistently missed by one of the tuning methods. 

%We compare the performance of the grammar in its original state, and
%the cleaned up version. 

\subsection{Final order}

After checking the conflicts and redundancies of the grammar the
linguists will proceed to reorder the grammar by defining the sections
of the grammar corresponding to each level of granularity of the Basque tag set.



\section{Evaluation}

We evaluate the grammars with a manually disambiguated corpus of 65,153 tokens/53,429 words,
compiled from different sources \cite{aduriz2006epec}.
We report the original score, and the result from ML-tuning the original grammar, 
as well as the result of preliminary cleanup. 
The scores are given using two metrics, differing on the granularity of the tagset.

\subsection{Evaluation criteria}

The Basque tag set is structured in four levels of granularity. 
As explained in \newcite{ezeiza1998basque}, the first level contains only the main POS, 20 distinct tags,
and the fourth level contains several hundreds of tags with fine-grained distinctions, including semantic tags such as animacy.
Table~\ref{table:levels} shows a simplified example of the levels for nouns.
On the 4th level, the initial ambiguity is very high: the test corpus has, on average,
3.96 readings per cohort. On the 2nd level, when readings that differ only in 
higher-level tags are collapsed into one, the initial ambiguity is 2.41 readings per cohort.
We follow the scheme for evaluation: assume that we are left with two readings, ``Common noun, singular''
and ``Common noun, plural'', and one of them is correct. Evaluation on levels 3 and 4 reports 100 \% recall and 50 \% precision.
Evaluation on levels 1 and 2 ignores the tags from the higher levels, and regards any common noun or noun as correct, hence 100 \% for both measures. 

It should be noted that the linguistic revision has been targeted
towards improving the 2nd level. 
%Nevertheless, the score on both levels improves


\begin{table*}[t]
\centering
\begin{tabular}{llll}

\hline
\multicolumn{1}{|l|}{\textbf{Level 1}} & \multicolumn{1}{l|}{\textbf{Level 2}}                                                  & \multicolumn{1}{l|}{\textbf{Level 3}}                                                                                                                                                          & \multicolumn{1}{l|}{\textbf{Level 4}}                                                                                                                                                                                                        \\ \hline
\multicolumn{1}{|l|}{Noun}             & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Common noun\\ Proper noun\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Common noun, plural absolutive\\ Common noun, singular ergative\\ Proper noun, plural absolutive\\ Proper noun, singular ergative\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Common noun, plural absolutive, animate\\ Common noun, plural absolutive, inanimate\\ ...\\ Proper noun, singular ergative, animate\\ Proper noun, singular ergative, inanimate\end{tabular}} \\ \hline

\end{tabular}
\caption{Levels of granularity}
\label{table:levels}
\end{table*}



\subsection{Analysis of the results}
\label{analysis}

The results of the preliminary evaluation are in Table~\ref{table:res}. 
The drop in performance after the preliminary cleanup is most certainly due to ordering---we found it 
easier to work on the grammar directly after grouping and sorting the rules, as shown in
Figure~\ref{fig:sorted}. 
%Both precision and recall get worse, 
% The worst drop is in precision; from 62 \% to 56 \% on the full tagset,
%which suggests that the alphabetical ordering blocks many rules from firing.
ML-tuning the cleaned grammar brings the precision up, indicating that
more rules get to fire in the tuned order. The difference is most
dramatic in the sorted and grouped grammar on the 4th level: the
original precision drops from 62 \% to 56 \%, and goes up to 68 \%
with the ML-tuning. 

As explained in Section~\ref{manual}, the fairest test at this stage is to compare the
ML-tuned results of the original and the cleaned grammar.
We see the cleaned and tuned grammar slightly outperforming the tuned original;
the difference is not large, but we see it as a promising start.



%Given that recall only improves 2 percentage points, but precision more than 20, the main problem
%seems to be not disambiguating enough, and to only smaller extent removing the right reading.




\begin{table*}[t]
\centering
\begin{tabular}{l|l|l|l|l|l|l|}
\cline{2-7}
                                                 & \multicolumn{3}{l|}{\textbf{All tags} (Level 4)}            & \multicolumn{3}{l|}{\textbf{48 main categories} (Level 2)}  \\ \cline{2-7} 
                                                 & \textit{Rec.} & \textit{Prec.} & \textit{F-score} & \textit{Rec.} & \textit{Prec.} & \textit{F-score} \\ \hline
\multicolumn{1}{|l|}{\textbf{Original grammar}}     
					    & \textbf{95.61}  & 62.99         &  75.94         & \textbf{97.48}  &  84.37           &  90.45           \\ \hline
\multicolumn{1}{|l|}{\textbf{ML-tuned original}}    
					    & 93.87          & 68.06             &  78.91              & 96.66          &  86.82           &  91.48            \\ \hline
\multicolumn{1}{|l|}{\textbf{Preliminary cleanup}}  
					   & 94.81           & 56.56             & 70.85               &  96.82         &  84.13           &  90.03           \\ \hline
\multicolumn{1}{|l|}{\textbf{ML-tuned prel.cl.}}      
					   & 93.41           & \textbf{68.61}  & \textbf{79.11}   &  96.41        &  \textbf{87.19}   &  \textbf{91.57}           \\ \hline
\end{tabular}
\caption{Preliminary evaluation on words, excluding punctuation, for levels 4 and 2 of granularity.}
\label{table:res}
\end{table*}

\paragraph{Conflict check}

%We have worked on a reduced set of 21,000 readings, tailored to address this particular grammar. 
%Unfortunately, 21,000 readings is at the limit of practical: the program can check up to 200 rules at a time, before it uses too much memory on a 4-year-old Macbook Air (1,7 GHz Intel Core i7, 8 GB memory). 

Our main problem is the size of the tag set: all possible combinations of tags on level 4 amount to millions of readings,
and that would make the SAT-problems too big.
We cannot just ignore all tags beyond level 2 or 3, because many of the rules rely on them as contexts.

As a first approximation, we have created a reduced set of 21000 readings, which allows the program to check up to 200 
rules at a time before running out of memory. We are still developing better solutions, and have not run the whole grammar with this setup.
Among the first rules we tested, it has found a few redundancies, such as the following:

\begin{verbatim}
  SELECT ADB IF
    (0 POSTPOSIZIOAK-9) 
    (-1 IZE-DET-IOR-ADJ-ELI-SIG + INE) ;
  SELECT ADB IF  
    (0 ("<barrena>")) (-1 INE) ;
\end{verbatim}

\noindent The problem is that the set \texttt{POSTPOSIZIOAK-9} contains the word form ``barrena'', and the other set contains the tag \texttt{INE}; in other words, the latter rule is fully contained in the first rule and hence redundant.


Our second strategy is to reduce the rules themselves: from a rule such as \texttt{SELECT Verb + Sg IF (1 Noun + Sg)},
we just remove all tags higher than level 2, resulting in
\texttt{SELECT Verb IF (1 Noun)}. We also keep all lexical tags
intact, but unlike in \newcite{listenmaa_claessen2016}, we allow them to attach to any morphological tags; this may
lead to further false negatives, but reduces the size of the SAT-problem.
This setup analyses the whole grammar, in the given order, in approximately 1 hour. 
With the reduced rules, the program would not find the redundancy described earlier, because
the problem lies in the 3rd-level tag \texttt{INE}. But this approximation found successfully 11 duplicates or near-duplicates in the whole grammar, such as the following:

\begin{verbatim}
SELECT IZE IF 	 # line 817
  (0 POSTPOSIZIOAK-10IZE LINK 0 IZE_ABS_MG) 
  (-1 IZE-DET-IOR-ADJ-ELI-SIG + GEN) ; 
SELECT IZE IF 	 # line 829
  (0 POSTPOSIZIOAK-10IZE + IZE_ABS_MG) 
  (-1 IZE-DET-IOR-ADJ-ELI-SIG + GEN) ;
\end{verbatim}

\noindent Both of the contextual tests contain 3rd-level tags (\texttt{ABS, MG, GEN}), but removing them keeps the sets identical, hence it is not a problem for the conflict check.



Finally, all setups have found some internal conflicts. In order to get a more reliable account, we would need 
more accurate tagset, beyond the 21000. To be fair, many internal conflicts can be detected by simpler means:
using STRICT-TAGS would reveal illegal tags, which are the reason for a large number of internal conflicts. 
But some cases are due to a mistake in logic, rather than a typo; examples such as the following were easily found by the tool. %elow are some examples of the internal conflicts found by the tool:

\begin{verbatim}
  REMOVE ADI IF (NOT 0 ADI) (1 BAT) ;
  SELECT ADI IF (0C ADI LINK 0 IZE) ;
\end{verbatim}

\noindent The first rule is clearly an error; it is impossible to remove an ADI from a reading that does not have one.
The conflict likely stems from a confusion between \texttt{NOT X} and \texttt{(*) - X};
The second rule is not obvious to the eye; the interplay of \texttt{0C} and \texttt{LINK 0} requires 
\texttt{ADI} and \texttt{IZE} in the same reading, which is not possible\footnote{ADI is a verb, IZE is a noun}.

%We have found the conflict check helpful for the grammarians: it is useful information to know about redundancies. But so far

\paragraph{ML-tuning}

So far, the most important use of the ML-tuning has been to overcome
the differences in ordering. 
Given the preliminary nature of the work, we have not tried multiple
variations. We used a development corpus of 61,524
tokens and a test corpus of 65,153 tokens; the same which we used to
obtain the scores in Table~\ref{table:res}. We stopped the tuning
after 5 iterations, and used an error threshold of 25 \% to consider a
rule as ``good'' or ``bad''. 

In the future, as the grammar cleanup advances, we are interested in
trying out different settings.
% as we approach the task of finding an optimal order. 
Already in our current stage, ML-tuning has clearly
improved the precision, for both original and preliminarily cleaned
grammars, and for both levels of granularity; it is likely that
experimenting with different parameters, we would find a combination
that would also improve the recall, like \newcite{bick2013tuning} 
and \newcite{bick_hagen2015obt} report.
However, while ML-tuning improves the grammar's performance, it makes
it less readable for human eyes, and continuing the development is
harder. Thus we might settle to two versions of the grammar: one for
maintenance, and other for running.




% \section{Future work}

% We have identified a number of features that might help the grammar
% writing process. The following list is mostly speculation, or more of
% a wish list, rather than us planning to implement all of this.

% %The following techniques are for now just speculation.

% %They can work either with a corpus, or a good description of possible ambiguity classes in the language.

% \subsection{Minimisation of tags used in the rules}

% Take a test corpus/ambiguity classes, look at the readings, 
% group tags by those that discriminate between correct and incorrect readings.
% Example:

% \begin{verbatim}
% "<lurtarraren>"
%   "lurtar" ADJ ARR IZAUR+ GEN 
%            NUMS MUGM ZERO <Correct!>
%   "lurtar" IZE ARR GEN NUMS MUGM ZERO 
%   "lurtar" ADJ ARR IZAUR+ ABS MG
% \end{verbatim}

% For the given cohort, tags that are only in the correct are \t{GEN} and only in incorrect are \t{IZE, ABS, MG}.
% In other words, we learn that a rule that would target e.g. \t{ZERO} or \t{IZAUR+} would not remove all ambiguity.
% We can compute these tags for all cohorts/ambiguity classes, and see if some rules in the grammar could be simplified.


% \subsection{Views on grammar}

% There are many ways to order the rules in a grammar. For better maintainability, 
% it makes sense to place all rules that target certain phenomena together.
% On the other hand, some rules may be very heuristic, and don't make sense to be in the same section.

% We propose a solution of different \emph{views} on the grammar.
% \begin{itemize}
% \item Base order: Whichever criteria that make the grammar
%   maintainable. The rest of the views can be generated from this view.
% \item Running order: can be optimised with ML-tuning, no need to be human-readable
% %\item Sorted by feature: for the ease of the grammarian to find all rules that target the same phenomena.
% %For example, ``all rules that target nouns'', ``all rules that mention postpositions in their contextual tests''.
% \item Tree view of rule dependencies. See next section about that.
% \end{itemize}



% \subsubsection{Tree view of rule dependencies}

% Two rules may be connected in several ways:

% \begin{itemize}
% \item Contradiction: Rules X and Y, in any order, give explicitly different instructions. Example: X = \t{REMOVE Noun} and Y = \t{SELECT Noun}.
% \item ``Feeding order'': Rule X earlier in the sequence enables a later rule Y. Example: X = \t{REMOVE Noun} and Y = \t{SELECT Adj IF (-1 NOT Noun)}
% \item ``Bleeding order'': Rule X earlier in the sequence blocks a later rule Y. Example:  X = \t{REMOVE Noun} and Y = \t{SELECT Adj IF (-1 Noun)}.
% \item Counter(f/bl)eeding order: rules X and Y are reversed. Counterbleeding order is optimal, given that both rules are good: only in this order, both Y and X have a chance to apply. Counterfeeding is suboptimal but tolerable: X will act on the first iteration, and Y only on the second iteration, unless there is another rule that would block Y before.
% \item Rule X removes both correct and incorrect readings. Rule Y, if placed before X, renders X unable to act in some cases. Ideally, in the incorrect cases!
% \end{itemize}

% The rules could be arranged in the form of a tree, based on how they affect and are affected by other rules. Combined with the feature view, a grammarian could have a look at all rules that target nouns and their dependencies.

\section{Conclusions and future work}

After checking the soundness of the grammar by means of some simple
tools, we are aware that in the near future we will need more complex
utilities for helping the grammar writing.
The following items are on our wish list:

\begin{itemize}
\item Flexible rule ordering, possibly implemented as different views
  in the CG IDE. The base order would be one that is easily
  maintainable and linguistically motivated, and any other orders can
  be generated from the base order.
\item Finding deeper connections between the rules. So far we have used the
  SAT-based conflict check to run the grammar in order, but we would
  like to develop this further: take any given rule, and give a list
  of all the rules, anywhere in the grammar, that potentially feed to
  it or block it. The biggest problem in developing such a method is
  the size of the tagset. 
\item Tagset minimisation: it is possible that some of the level 4 or
  3 tags used in the rules could be removed without it affecting the
  functionality of the grammar. Using a development corpus, we could
  find the minimal set of tags that discriminate between correct and
  incorrect readings.
Example:

\begin{verbatim}
"<lurtarraren>"
  "lurtar" ADJ ARR IZAUR+ GEN 
           NUMS MUGM ZERO <Correct!>
  "lurtar" IZE ARR GEN NUMS MUGM ZERO 
  "lurtar" ADJ ARR IZAUR+ ABS MG
\end{verbatim}

For the given cohort, tags that are only in the correct are \t{GEN} and only in incorrect are \t{IZE, ABS, MG}.
In other words, we learn that a rule that would target e.g. \t{ZERO} or \t{IZAUR+} would not remove all ambiguity.
We can compute these tags for all cohorts/ambiguity classes, and see if some rules in the grammar could be simplified.
\end{itemize}

TODO: some final remarks

\section*{Acknowledgments}

This work has been supported by the project UPV/EHU taldea. UPV/EHU (GIU16/16)





% \section*{Acknowledgments}

% Do not number the acknowledgment section. Do not include this section
% when submitting your paper for review.


\bibliographystyle{acl}
\bibliography{cg}


\end{document}
