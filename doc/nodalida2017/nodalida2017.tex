%
% File nodalida2017.tex
%
% Contact beata.megyesi@lingfil.uu.se
%
% Based on the instruction file for Nodalida 2015 and EACL 2014
% which in turn was based on the instruction files for previous 
% ACL and EACL conferences.

\documentclass[11pt]{article}
\usepackage{nodalida2017}
%\usepackage{times}
\usepackage{mathptmx}
%\usepackage{txfonts}
\usepackage{url}
\usepackage{latexsym}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to

\title{Cleaning up the Basque grammar: a work in progress}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

% \begin{abstract}
%   This document contains the instructions for preparing a camera-ready
%   manuscript for the proceedings of Nodalida-2017. The document itself
%   conforms to its own specifications, and is therefore an example of
%   what your manuscript should look like. These instructions should be
%   used for both papers submitted for review and for final versions of
%   accepted papers.  Authors are asked to conform to all the directions
%   reported in this document.
% \end{abstract}


\section{Introduction}

The Basque CG was originally written in 19XX, using the CG-1 formalism. Since then, it has undergone many changes, by many grammarians. The file consists of 8600 lines, out of which 2600 are rules or tagsets--rest are comments and commented out rules or sets. During the two decades of development, the Basque morphological analyser has also been updated several times, and not always synchronised with the CG. As a result, the Basque grammar needs serious attention.

In the present paper, we describe the ongoing process of cleaning up the Basque grammar. We use a variety of tools and methods, ranging from simple string replacements to SAT-based symbolic evaluation, introduced in \newcite{listenmaa_claessen2016}, and grammar tuning by \newcite{bick2013tuning}. We present our experiences in combining all these tools, along with a few modest additions to the simpler end of the scale.


\section{Previous work}

\newcite{bick2013tuning} presents a method for tuning a grammar, based on machine learning. Bick reports an error reduction between 7--15 \% when tested on the Danish tagging grammar.
\newcite{listenmaa_claessen2016} present a method for detecting contradictions in a grammar, using SAT-based symbolic evaluation.  They report detecting rule conflicts in a few small grammars, but provide no further evaluation on the grammars after fixing the rule conflicts. 
In our experiments, we use both of these tools for different purposes, complementing each other.

\section{Pipeline}

As a first step, we run a series of simple, mostly off-the-shelf tools. 
The next step is to group the rules and order them by their contextual tests.
These sets are checked both by the SAT-based tool, and grammarians. After these
steps, we give the grammar as an input for ML-tuning.


\subsection{Simple tools}

\paragraph{String operations}
Fix typos: O for 0, and various mismatched \texttt{"<>"} in word forms: e.g. \texttt{"<zuen">}, \texttt{<argi>"}.
Transform word forms into case-insensitive, remove duplicates. 
There were many occurrences of identical rules, of the form \texttt{REMOVE ("<x>")} and \texttt{REMOVE ("<X>")}. We changed those rules into the form \texttt{REMOVE ("<x>"i)}, and removed duplicate rules after that.

\paragraph{Tagset operations}
The VISL CG-3 compiler offers useful features, such as \texttt{--show-unused-sets} and \texttt{--show-tags}. With the former, we could eliminate 255 unused tagsets, and with the latter, we detected 15 obsolete or misspelled tags in the remaining used tagsets, by comparing against an up-to-date morphological lexicon.

% A special case, which occurred 4 of the 15 times, was to use a set in place of a tag name, such as in Figure~\ref{fig:tagnames}.

% \begin{figure}
% \caption{Tagsets used as tag names}
% \label{fig:tagnames}
% \begin{verbatim}
% LIST ADI/ADT = ADI ADT ;
% LIST ADIZEHAR = ("jakin" ADI/ADT) ;

% LIST MOD/DENB = MOD DENB ;
% REMOVE (BALD ERL MOD/DENB) ;
% \end{verbatim}
% \end{figure}

% Both of the lists, ADIZEHAR and the inline target in the remove rule, are using a previously defined list in a position where there should be a tag name instead. As a result, ADI/ADT and MOD/DENB are interpreted as tag names---such tag names don't exist, and hence these rules can never apply.

\paragraph{Human readability}
For improving the readability of the grammar, we wrote a tool that finds repetitive set definitions, and suggests ways to compact them. An example is shown below:

\begin{itemize}
\item [Original definition:] \texttt{("ageri" ADJ ABS MG) ("bizi" ADJ ABS MG) ... ("haizu" ADJ ABS MG) ;}
\item [Compact version:] \texttt{("ageri"|"bizi"|"haizu") + (ADJ ABS MG) ;}

\end{itemize}



In addition, the grammar contains many rules that specify an inline set, when there is already the same or a very similar set definition. For instance, the rule \texttt{REMOVE (ADL) IF (0 ADT) (1 ("<.>") OR ("<;>") OR ("<,>") OR ("<:>") OR ("<?>") OR ("<!>"))} lists different punctuation marks as word forms, instead of using the list PUNTUAZIOA, which contains all these tokens.

The standard tools did not provide this type of suggestions, so we wrote these tools ourselves. Neither of these transformations is applied automatically, they are just suggestions for the grammar writers.



\subsection{Group by target, sort by conditions}
After the simple checks and transformations, we group the rules by their targets, and sort them by the complexity of their contextual texts. For instance, the 5 rules that target ADOIN will be in the order shown in Figure~\ref{fig:sorted}: from fewest to most contextual tests, and in the case of same number of tests, preferring those with fewer tagsets.

\begin{figure*}[ht]
\begin{minipage}{\textwidth}
\label{fig:sorted}
\begin{verbatim}
SELECT ADOIN IF (1 ARAZI) ;                           # line 7412
REMOVE ADOIN IF (0 IZE) (1C ADJ) ;                    # line 6423
REMOVE ADOIN IF (0 IZE) (1 DET | ADJ | IZE) ;         # line 6433
REMOVE ADOIN IF (0 EZEZAG + ADJ + GEN) (-2C IZE) ;    # line 6319
REMOVE ADOIN IF (0 IZE) (-1C IZE) (1C ADJ) ;          # line 6422
\end{verbatim}
\caption{Rules grouped by target, and ordered by their contextual tests.}
\end{minipage}
\end{figure*}
\enlargethispage{-4\baselineskip}

\subsection{Check for conflicts and redundancies}

When the rules are grouped and sorted as described, we run SAT-based 
symbolic evaluation \cite{listenmaa_claessen2016} on each group. 
If it says that some rule with a more complex condition is superfluous 
because of another rule earlier in the list\footnote{For example, the latter rule of the following is superfluous: \texttt{REMOVE Verb IF (-1 Det)} and \texttt{REMOVE Verb IF (-1 Det) (1 Verb)}}, 
then that is a hint for the grammar writer: why are there two similar 
rules in the grammar, if the simpler one would do?
At the moment of writing this abstract, we are still working on adding
the SAT-method to the pipeline, and have no concrete results to report.

Even if the SAT-method wouldn't find a conflict, we give the rules to 
a grammarian in any case. The grammarian works with this list, having 
the original grammar on the side to see the comments, or other original 
context of any given rule. Seeing all the rules grouped helps with the 
situation where different grammarians have written rules independent of 
each other. The grammarian can do their manual modifications either on 
the original grammar, or to this grouped version.

This sorting and grouping is not meant to be the final order, it is only 
to help a human grammarian to make decisions regarding all the rules that 
target the same tagsets.

\subsection{Ordering: ML-tuning}
Once the grammar has gone through the previous steps, we give it to the 
ML-tuning tool \cite{bick2013tuning}, with the purpose of finding an 
optimal order. Then we can run the newly ordered grammar through SAT-evaluation,
to detect if the ML-tuning has introduced new conflicts or superfluous rules.

Our initial hypothesis is that the human-cleaned version will benefit more 
from tuning than the original grammar. Some bad rules may have only a minor 
problem, such as a single tag name having changed meaning, and they would be 
better fixed by updating the obsolete tag, instead of the whole rule being 
demoted or killed. 
To test our assumptions, we run ML-tuning for both the original grammar and 
the human-cleaned version: it is also possible that this error type is not very 
common, and the majority of the bad rules were bad already when they were born.

% We are also interested to find out whether there are some rules or rule types 
% that are consistently missed by one of the tuning methods. 

%We compare the performance of the grammar in its original state, and the cleaned up version. 

\section{Evaluation}




% \section*{Acknowledgments}

% Do not number the acknowledgment section. Do not include this section
% when submitting your paper for review.


\bibliographystyle{acl}
\bibliography{cg}


\end{document}
