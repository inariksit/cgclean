%
% File nodalida2017.tex
%
% Contact beata.megyesi@lingfil.uu.se
%
% Based on the instruction file for Nodalida 2015 and EACL 2014
% which in turn was based on the instruction files for previous 
% ACL and EACL conferences.

\documentclass[11pt]{article}
\usepackage{nodalida2017}
%\usepackage{times}
\usepackage{mathptmx}
%\usepackage{txfonts}
\usepackage{url}
\usepackage{latexsym}
\special{papersize=210mm,297mm} % to avoid having to use "-t a4" with dvips 
%\setlength\titlebox{6.5cm}  % You can expand the title box if you really have to

\title{Cleaning up the Basque grammar: a work in progress}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   {\tt email@domain} \\}

\def\t#1{\texttt{#1}}

\date{}

\begin{document}
\maketitle

% \begin{abstract}
%   This document contains the instructions for preparing a camera-ready
%   manuscript for the proceedings of Nodalida-2017. The document itself
%   conforms to its own specifications, and is therefore an example of
%   what your manuscript should look like. These instructions should be
%   used for both papers submitted for review and for final versions of
%   accepted papers.  Authors are asked to conform to all the directions
%   reported in this document.
% \end{abstract}


\section{Introduction}

The first version of the Basque Constraint Grammar (BCG) was developed in 1995--1997 by two linguists \cite{aduriz1997euscg} based on the Constraint Grammar theory of \newcite{karlsson1990cgp,karlsson1995constraint}.
Since then, it has undergone many changes, by many grammarians.
% The file consists of 8600 lines, out of which 2600 are rules or tagsets--rest are comments and commented out rules or sets. 
During the two decades of development, the Basque morphological analyser has also been updated several times, and not always synchronised with the CG. As a result, the Basque grammar needs serious attention.

In the present paper, we describe the ongoing process of cleaning up the Basque grammar. We use a variety of tools and methods, ranging from simple string replacements to SAT-based symbolic evaluation, introduced in \newcite{listenmaa_claessen2016}, and grammar tuning by \newcite{bick2013tuning}. We present our experiences in combining all these tools, along with a few modest additions to the simpler end of the scale.


\section{Previous work}

\newcite{bick2013tuning} presents a method for automatically tuning a grammar, and reports an error reduction between 7--15 \% when tested on the Danish tagging grammar.
\newcite{listenmaa_claessen2016} present a method for detecting contradictions in a grammar, using SAT-based symbolic evaluation.  They report detecting rule conflicts in a few small grammars, but provide no further evaluation on the grammars after fixing the rule conflicts. 
In our experiments, we use both of these tools for different purposes, complementing each other.

\section{Pipeline}

As a first step, we run a series of simple, mostly off-the-shelf tools. 
The next step is to group the rules and order them by their contextual tests.
These sets are checked both by the SAT-based tool, and grammarians. After these
steps, we give the grammar as an input for ML-tuning.


\begin{figure*}[t]
\centering
\label{fig:sorted}
\begin{tabular}{lr}
\t{SELECT ADOIN IF (1 ARAZI) ;}                         & \t{\# line 7412} \\
\t{REMOVE ADOIN IF (0 IZE) (1C ADJ) ;}                  & \t{\# line 6423} \\
\t{REMOVE ADOIN IF (0 IZE) (1 DET | ADJ | IZE) ;}       & \t{\# line 6433} \\
\t{REMOVE ADOIN IF (0 EZEZAG + ADJ + GEN) (-2C IZE) ;}  & \t{\# line 6319} \\
\t{REMOVE ADOIN IF (0 IZE) (-1C IZE) (1C ADJ) ;}        & \t{\# line 6422} \\
\end{tabular}
\caption{Rules grouped by target, and ordered by their contextual tests.}

\end{figure*}



\subsection{Simple tools}

\paragraph{String operations}
Fix typos: O for 0, and various mismatched \texttt{"<>"} in word forms: e.g. \texttt{"<zuen">}, \texttt{<argi>"}.
Transform word forms into case-insensitive, remove duplicates. 
There were many occurrences of identical rules, of the form \texttt{REMOVE ("<x>")} and \texttt{REMOVE ("<X>")}. We changed those rules into the form \texttt{REMOVE ("<x>"i)}, and removed duplicate rules after that.

\paragraph{Tagset operations}
The VISL CG-3 compiler offers useful features, such as \texttt{--show-unused-sets} and \texttt{--show-tags}. With the former, we could eliminate 255 unused tagsets, and with the latter, we detected 15 obsolete or misspelled tags in the remaining used tagsets, by comparing against an up-to-date morphological lexicon.

% A special case, which occurred 4 of the 15 times, was to use a set in place of a tag name, such as in Figure~\ref{fig:tagnames}.

% \begin{figure}
% \caption{Tagsets used as tag names}
% \label{fig:tagnames}
% \begin{verbatim}
% LIST ADI/ADT = ADI ADT ;
% LIST ADIZEHAR = ("jakin" ADI/ADT) ;

% LIST MOD/DENB = MOD DENB ;
% REMOVE (BALD ERL MOD/DENB) ;
% \end{verbatim}
% \end{figure}

% Both of the lists, ADIZEHAR and the inline target in the remove rule, are using a previously defined list in a position where there should be a tag name instead. As a result, ADI/ADT and MOD/DENB are interpreted as tag names---such tag names don't exist, and hence these rules can never apply.

\paragraph{Human readability}
For improving the readability of the grammar, we wrote a tool that finds repetitive set definitions, and suggests ways to compact them. An example is shown below:

\begin{table}[h]
\begin{tabular}{l}
Original  \\
 ~~~~~~~~\texttt{("ageri" ADJ ABS MG)} \\
 ~~~~~~~~\texttt{("bizi" ADJ ABS MG) ...} \\
 ~~~~~~~~\texttt{("haizu" ADJ ABS MG) ;} \\
Compact \\
 ~~~~~~~~\texttt{("ageri"|"bizi"|"haizu") +} \\
 ~~~~~~~~\texttt{(ADJ ABS MG) ;}

\end{tabular}
\end{table}


In addition, the grammar contains many rules that specify an inline set, when there is already the same or a very similar set definition. For instance, the rule \texttt{REMOVE (ADL) IF (0 ADT) (1 ("<.>") OR ("<;>") OR ("<,>") OR ("<:>") OR ("<?>") OR ("<!>"))} lists different punctuation marks as word forms, instead of using the list PUNTUAZIOA, which contains all these tokens.

The standard tools did not provide this type of suggestions, so we wrote these tools ourselves. Neither of these transformations is applied automatically, they are just suggestions for the grammar writers.



\subsection{Group by target, sort by conditions}
After the simple checks and transformations, we group the rules by their targets, and sort them by the complexity of their contextual texts. For instance, the 5 rules that target ADOIN will be in the order shown in Figure~\ref{fig:sorted}: from fewest to most contextual tests, and in the case of same number of tests, preferring those with fewer tagsets.


\subsection{Check for conflicts and redundancies}

When the rules are grouped and sorted as described, we run SAT-based 
symbolic evaluation \cite{listenmaa_claessen2016} on each group. 
If it says that some rule with a more complex condition is superfluous 
because of another rule earlier in the list\footnote{For example, the latter rule of the following is superfluous: \texttt{REMOVE Verb IF (-1 Det)} and \texttt{REMOVE Verb IF (-1 Det) (1 Verb)}}, 
then that is a hint for the grammar writer: why are there two similar 
rules in the grammar, if the simpler one would do?
At the moment of writing this abstract, we are still working on adding
the SAT-method to the pipeline, and have no concrete results to report.

Even if the SAT-method wouldn't find a conflict, we give the rules to 
a grammarian in any case. The grammarian works with this list, having 
the original grammar on the side to see the comments, or other original 
context of any given rule. Seeing all the rules grouped helps with the 
situation where different grammarians have written rules independent of 
each other. The grammarian can do their manual modifications either on 
the original grammar, or to this grouped version.

This sorting and grouping is not meant to be the final order, it is only 
to help a human grammarian to make decisions regarding all the rules that 
target the same tagsets.

\subsection{Ordering: ML-tuning}
Once the grammar has gone through the previous steps, we give it to the 
ML-tuning tool \cite{bick2013tuning}, with the purpose of finding an 
optimal order. Then we can run the newly ordered grammar through SAT-evaluation,
to detect if the ML-tuning has introduced new conflicts or superfluous rules.

Our initial hypothesis is that the human-cleaned version will benefit more 
from tuning than the original grammar. Some bad rules may have only a minor 
problem, such as a single tag name having changed meaning, and they would be 
better fixed by updating the obsolete tag, instead of the whole rule being 
demoted or killed. 
To test our assumptions, we run ML-tuning for both the original grammar and 
the human-cleaned version: it is also possible that this error type is not very 
common, and the majority of the bad rules were bad already when they were born.

% We are also interested to find out whether there are some rules or rule types 
% that are consistently missed by one of the tuning methods. 

%We compare the performance of the grammar in its original state, and the cleaned up version. 

\section{Evaluation}

We evaluate the grammars with a manually disambiguated corpus of 65,153 tokens/53,429 words,
compiled from different sources \cite{aduriz2006epec}.
So far we only report the original score, and the result from ML-tuning the original grammar.
Later, we will include also the version cleaned by SAT and human attention, as well as
the result of ML-tuning that.

\subsection{Evaluation criteria}

The Basque tag set is very fine-grained, with XXX distinct tags appearing in the corpus,
and the initial ambiguity is very high: the test corpus has, on the average,
6.7(?) readings per cohort. 

For this reason, we report two variants of the common measures,
shown in Table~\ref{table:res}.
The first variant is calculated in the standard way: all readings that 
are not the uniquely correct one, reduce the precision. 
The second variant operates on a reduced set of 50 tags, ignoring any additional tags.
For instance, given an initial ambiguity \t{<noun><sg><Correct!>/<noun><pl>/<verb>}, 
only removing the verb reading counts as 100 \% precision, even though the cohort is
not fully disambiguated with regard to number.



\subsection{Analysis of results}
Given that recall only improves 2 percentage points, but precision more than 20, the main problem
seems to be not disambiguating enough, and to only smaller extent removing the right reading.




\begin{table*}[t]
\centering
\label{my-label}
\begin{tabular}{l|l|l|l|l|l|l|}
\cline{2-7}
                                                 & \multicolumn{3}{l|}{\textbf{Standard}}            & \multicolumn{3}{l|}{\textbf{50 main categories}}  \\ \cline{2-7} 
                                                 & \textit{Rec.} & \textit{Prec.} & \textit{F-score} & \textit{Rec.} & \textit{Prec.} & \textit{F-score} \\ \hline
\multicolumn{1}{|l|}{\textbf{Original grammar}}  & 95.30       & 63.35        & 76.11    &  97.30        &  84.69         &  90.56           \\ \hline
\multicolumn{1}{|l|}{\textbf{ML-tuned original}} & 93.41       & 68.61        & 79.11    &  96.41        &  87.19         &  91.57           \\ \hline
\end{tabular}
\caption{Baseline evaluation with the two metrics.}
\label{table:res}
\end{table*}






\section{Experimental features}

The following techniques are for now just speculation.

They can work either with a corpus, or a good description of possible ambiguity classes in the language.

\subsection{Minimisation of tags}


Take a test corpus/ambiguity classes, look at the readings, 
group tags by those that discriminate between correct and incorrect readings.
Example:

\begin{verbatim}
"<lurtarraren>"
  "lurtar" ADJ ARR IZAUR+ GEN 
           NUMS MUGM ZERO <Correct!>
  "lurtar" IZE ARR GEN NUMS MUGM ZERO 
  "lurtar" ADJ ARR IZAUR+ ABS MG
\end{verbatim}

For the given cohort, tags that are only in the correct are \t{GEN} and only in incorrect are \t{IZE, ABS, MG}.
In other words, we learn that a rule that would target e.g. \t{ZERO} or \t{IZAUR+} would not remove all ambiguity.
We can compute these tags for all cohorts/ambiguity classes, and see if some rules in the grammar could be simplified.


\subsection{Views on grammar}

There are many ways to order the rules in a grammar. For better maintainability, 
it makes sense to place all rules that target certain phenomena together.
On the other hand, some rules may be very heuristic, and don't make sense to be in the same section.

We propose a solution of different \emph{views} on the grammar.
\begin{itemize}
\item Running order: can be optimised with ML-tuning, no need to be human-readable
\item Sorted by feature: for the ease of the grammarian to find all rules that target the same phenomena.
For example, ``all rules that target nouns'', ``all rules that mention postpositions in their contextual tests''.
\item Tree view of rule dependencies. See next section about that.
\end{itemize}



\subsubsection{Tree view of rule dependencies}

Two rules may be connected in several ways:

\begin{itemize}
\item Contradiction: Rules X and Y, in any order, give explicitly different instructions. Example: X = \t{REMOVE Noun} and Y = \t{SELECT Noun}.
\item ``Feeding order'': Rule X earlier in the sequence enables a later rule Y. Example: X = \t{REMOVE Noun} and Y = \t{SELECT Adj IF (-1 NOT Noun)}
\item ``Bleeding order'': Rule X earlier in the sequence blocks a later rule Y. Example:  X = \t{REMOVE Noun} and Y = \t{SELECT Adj IF (-1 Noun)}.
\item Counter(f/bl)eeding order: rules X and Y are reversed. Counterbleeding order is optimal, given that both rules are good: only in this order, both Y and X have a chance to apply. Counterfeeding is suboptimal but tolerable: X will act on the first iteration, and Y only on the second iteration, unless there is another rule that would block Y before.
\item Rule X removes both correct and incorrect readings. Rule Y, if placed before X, renders X unable to act in some cases. Ideally, in the incorrect cases!
\end{itemize}

The rules could be arranged in the form of a tree, based on how they affect and are affected by other rules. Combined with the feature view, a grammarian could have a look at all rules that target nouns and their dependencies.








% \section*{Acknowledgments}

% Do not number the acknowledgment section. Do not include this section
% when submitting your paper for review.


\bibliographystyle{acl}
\bibliography{cg}


\end{document}
